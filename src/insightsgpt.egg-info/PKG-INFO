Metadata-Version: 2.4
Name: insightsgpt
Version: 1.1.0
Summary: A command-line tool for congressional insights.
Author-email: Your Name / Org <your@email.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/CongressionalInsights/InsightsGPT
Project-URL: Bug Tracker, https://github.com/CongressionalInsights/InsightsGPT/issues
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 3 - Alpha
Classifier: Environment :: Console
Classifier: Intended Audience :: Developers
Classifier: Topic :: Utilities
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: python-dotenv
Dynamic: license-file

# InsightsGPT

## Making Government Data Accessible and Actionable

**InsightsGPT** is an open-source project designed to provide transparent, easy-to-access insights into U.S. legislative, regulatory, and campaign finance activities. By leveraging the power of generative AI, InsightsGPT bridges the gap between complex datasets and the people who need them most. Whether you're a journalist, researcher, activist, or curious citizen, InsightsGPT empowers you to explore government data with ease.

---

## Prerequisites

-   Python 3.9 or higher.
-   `pip` (Python package installer) for installing dependencies.

---

## Quick Start

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/CongressionalInsights/InsightsGPT.git
    cd InsightsGPT
    ```
2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    # For development (to run tests, linters, etc.), install additional development dependencies:
    # pip install -r requirements-dev.txt 
    ```

3.  **Run an example script:**
    Fetch recent Federal Register documents related to "climate change":
    ```bash
    python scripts/fetch_fr.py documents-search --term "climate change" --per_page 5 --order newest
    ```

    **Example Output (stdout):**
    ```
    INFO: GET https://www.federalregister.gov/api/v1/documents.json?per_page=5&order=newest&conditions%5Bterm%5D=climate+change
    INFO: Saved JSON to data/documents_search_term_climate_change_per_page_5_order_newest.json
    ```
    (A new file `data/documents_search_term_climate_change_per_page_5_order_newest.json` will be created containing the search results).

For more detailed examples and advanced usage, please see our [Sample Workflows Guide](docs/Sample_Workflows.md) and the [Full Usage Guide](docs/USAGE_GUIDE_FOR_AI.md).

---

## Installation

### From PyPI (Python Package Index)
*Future Goal:* We aim to publish `insightsgpt` to PyPI. Once available, you will be able to install it using pip:
```sh
pip install insightsgpt
```
For now, please use one of the methods below.

### From Local Wheel (Development/Testing)
You can build and install `insightsgpt` from the source code:
1. Clone the repository:
   ```sh
   git clone https://github.com/CongressionalInsights/InsightsGPT.git
   cd InsightsGPT
   ```
2. Build the wheel:
   ```sh
   python -m build
   ```
3. Install the built wheel (the exact filename in `dist/` may vary depending on the version):
   ```sh
   # Example for version 1.1.0
   pip install dist/insightsgpt-1.1.0-py3-none-any.whl 
   ```
After installation, you can run the CLI tool:
```sh
insightsgpt --help
```

### Using Docker
A Docker image is available on GitHub Container Registry (GHCR).
1. Pull the latest image:
   ```sh
   docker pull ghcr.io/congressionalinsights/insightsgpt:latest
   ```
2. Run commands using the image. For example, to run the `validate` subcommand:
   ```sh
   docker run --rm -v /path/to/your/local/data:/app/data congressionalinsights/insightsgpt validate --input_folder data
   ```
   - Replace `/path/to/your/local/data` with the actual path to your data directory on your host machine.
   - The `-v` flag mounts your local data directory into the `/app/data` directory inside the container, which the application might expect. Adjust the source and target paths as necessary based on how the scripts access data.
   - `--rm` automatically removes the container when it exits.
To see the help menu:
   ```sh
   docker run --rm congressionalinsights/insightsgpt --help
   ```

---

## Configuration

For local development, API keys (if required by specific scripts in the future) and other configurable parameters can be managed using a `.env` file in the project root.

1.  **Create your `.env` file:**
    Copy the example file to `.env`:
    ```bash
    cp .env.example .env
    ```
2.  **Edit `.env`:**
    Open the `.env` file and add any necessary environment variables. For example:
    ```env
    # FEDERAL_REGISTER_API_KEY=your_actual_api_key_if_needed
    # OTHER_CONFIG_PARAM=some_value
    ```
    Currently, `scripts/fetch_fr.py` does not require an API key for the Federal Register API it uses. However, `python-dotenv` has been integrated to support environment-specific configurations easily if needed in the future.

**Important:** The `.env` file should **not** be committed to version control and is listed in `.gitignore`. Ensure you keep any sensitive information like API keys in your local `.env` file only.

---

## Static Dashboard

This project includes a static dashboard to provide a quick overview of project alerts and visualizations.
The dashboard is automatically generated and updated using MkDocs and GitHub Pages.

### Accessing the Dashboard

The dashboard is hosted on GitHub Pages. You can typically access it at the following URL format:
`https://<your-github-username-or-orgname>.github.io/<repository-name>/`

(Please replace `<your-github-username-or-orgname>` and `<repository-name>` with the actual values for this repository.)

For example, if your GitHub username is `octocat` and your repository is `my-project`, the URL would be:
`https://octocat.github.io/my-project/`

### How it Works

- The dashboard content is written in Markdown and managed in the `/docs` directory.
- PNG charts from the `/visualizations` directory and JSON alerts from the `/alerts` directory (produced by CI) are copied into the `/docs` directory.
- A GitHub Actions workflow defined in `.github/workflows/publish-docs.yml` automatically builds the MkDocs site and deploys it to the `gh-pages` branch whenever changes are pushed to the `main` branch.

---

## **Key Features**

### **GitHub Actions Workflows**

InsightsGPT employs robust GitHub Actions workflows to maintain high code quality, data accessibility, and security. The workflows include:

- **Linting** with `flake8`.
- **Security Scanning** with `bandit`.
- **Code Formatting** with `black`.
- **Test Coverage** with `pytest-cov`.
- **Fetch Federal Register Data** via embedded Python script.

---

### **Data Validation Workflow**
- **Purpose**: Ensures the integrity of JSON files in the `data/`.
- **Trigger**: Runs on new commits to `data/`.
- **Script Used**: `validate_data.py`
- **Output**: Logs validation results and flags issues.

### **Visualization Workflow**
- **Purpose**: Automatically generates charts and visual summaries from datasets.
- **Trigger**: Runs on new commits to `datasets/`.
- **Script Used**: `generate_visualizations.py`
- **Output**: Saves visualizations to `visualizations/`.

### **Keyword Monitoring Workflow**
- **Purpose**: Flags documents containing specific keywords.
- **Trigger**: Runs daily (schedule: midnight UTC).
- **Script Used**: `monitor_keywords.py`
- **Output**: Saves flagged results to `alerts/`.

The workflows are triggered automatically on code pushes, pull requests, and now also manually via `workflow_dispatch` events, ensuring every change is thoroughly validated.

---

### **Scripts**

#### 1. `validate_data.py`
- **Purpose**: Validates JSON data for structure and required fields.
- **Usage**:
  ```bash
  python scripts/validate_data.py --input_folder data/ --output_file logs/validation_results.json
  ```

  ---

### **Dependencies**

The `requirements.txt` file includes the following tools to support the workflows:

- `requests`
- `flake8`
- `pytest`
- `pytest-cov`
- `bandit`
- `black`

---

### **Fetch Federal Register Workflow**

#### Overview

The **"Fetch Federal Register Data"** workflow fetches data from the Federal Register API using an embedded Python script. This workflow dynamically queries Federal Register endpoints (e.g., documents, agencies, public inspection).

#### How to Run the Workflow

1. Navigate to the **Actions** tab in the GitHub repository.
2. Select the workflow titled **"Fetch Federal Register Data"**.
3. Click **Run workflow** to execute the workflow manually via `workflow_dispatch`.

#### Embedded Logic

The embedded Python script fetches data using pre-configured parameters:

- **Search Term**: `education`
- **Publication Date**: From `2023-01-01` onwards.
- **Result Fields**: `title`, `document_number`, `url`, `publication_date`

#### Output

- Results are saved as JSON files in the `data/` folder.
- Example file name: `federal_register_education.json`

---

### **How to Run the Code Quality Workflow**

1. Navigate to the **Actions** tab in the GitHub repository.
2. Select the workflow titled **"Code Quality, Security Scan, and Coverage"**.
3. Click **Run workflow** to execute the workflow manually.

---

### **ChangeLog**

#### Version 1.0.2

- Replaced `fetch_fr.py` logic with embedded Python script in the Federal Register workflow.
- Enabled `workflow_dispatch` for manual workflow triggering.
- Updated README to reflect new workflows and triggers.

#### Version 1.0.1

- Added GitHub Actions workflows for code quality, data fetching, and automation.
- Unified `fetch_fr.py` script to handle all Federal Register endpoints.
- Enhanced README with workflow instructions and examples.
- Implemented Dependabot for automatic dependency updates.

---

### **Repository Structure**

- **`scripts/`**: Contains Python scripts for interacting with government APIs, including `validate_data.py` and `monitor_keywords.py`.
- **`docs/`**: Reference files, guides, and structured metadata to help users and contributors.
- **`.github/workflows/`**: Workflow files for testing, deploying, and data fetching.

This repository combines automation, generative AI, and open-source collaboration to make U.S. government data accessible and actionable for everyone.
